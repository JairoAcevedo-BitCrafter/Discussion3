{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTSQNVAAz+MfMxdcm0oVeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JairoAcevedo-BitCrafter/Discussion3/blob/main/Discussion3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import of all librarys"
      ],
      "metadata": {
        "id": "oppAuc0rCSX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhK4DNCaCQ2P",
        "outputId": "8f929465-7f2b-4c75-944a-6f249631e0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "from num2words import num2words\n",
        "from matplotlib import pyplot as plt\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(text):\n",
        "  return text.lower()\n",
        "def rem_lines(text):\n",
        "    return text.strip().replace('\\n', ' ')\n",
        "def rem_tags(text):\n",
        "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "def fix_ct(text):\n",
        "    return contractions.fix(text)\n",
        "def rem_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  return [word for word in word_tokenize(text) if not word in stop_words]\n",
        "def rem_punct(text):\n",
        "    no_punct = [w.translate(str.maketrans('', '', string.punctuation)) for w in word_tokenize(text)]\n",
        "    return [word for word in no_punct if word.isalpha()]\n",
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "def to_number(text):\n",
        "    return(re.sub(r\"(\\d+)\", lambda x: num2words(int(x.group(0))), text))\n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS_EMO:\n",
        "        text = text.replace(emot, EMOTICONS_EMO[emot]+\" \".replace(\"\",\"\"))\n",
        "    return text\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMOJI:\n",
        "        text = text.replace(emot,UNICODE_EMOJI[emot]+\" \".replace(\"\",\"\"))\n",
        "    return text\n",
        "def clean_text(text):\n",
        "  text = text.lower() # convert to lowercase\n",
        "  text = re.sub(r'[^a-z0-9\\s]', '', text) # remove non-alphanumeric characters\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "Iud2ZVl2Dlu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain data from the train.csv added to the files in colab\n",
        "data=pd.read_csv('train.csv',sep=\",\")\n",
        "# Take out the Survived column\n",
        "features = data.drop(columns=['Survived'])\n",
        "# Convert all columns to string\n",
        "features = features.astype(str)\n",
        "# Concatenate all rows\n",
        "data['text'] = features.apply(lambda x: ' '.join(x), axis=1)\n",
        "print(data['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L0r0YnScMh_",
        "outputId": "d52cc9e3-d590-4242-987f-95843d9b19cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      1 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 ...\n",
            "1      2 1 Cumings, Mrs. John Bradley (Florence Brigg...\n",
            "2      3 3 Heikkinen, Miss. Laina female 26.0 0 0 STO...\n",
            "3      4 1 Futrelle, Mrs. Jacques Heath (Lily May Pee...\n",
            "4      5 3 Allen, Mr. William Henry male 35.0 0 0 373...\n",
            "                             ...                        \n",
            "886    887 2 Montvila, Rev. Juozas male 27.0 0 0 2115...\n",
            "887    888 1 Graham, Miss. Margaret Edith female 19.0...\n",
            "888    889 3 Johnston, Miss. Catherine Helen \"Carrie\"...\n",
            "889    890 1 Behr, Mr. Karl Howell male 26.0 0 0 1113...\n",
            "890    891 3 Dooley, Mr. Patrick male 32.0 0 0 370376...\n",
            "Name: text, Length: 891, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning\n",
        "data['text']=data['text'].astype(str).apply(rem_lines)\n",
        "data['text']=data['text'].astype(str).apply(rem_tags)\n",
        "#data['text']=data['text'].astype(str).apply(convert_emoticons)\n",
        "#data['text']=data['text'].astype(str).apply(convert_emojis)\n",
        "#data['text']=data['text'].astype(str).apply(fix_ct)\n",
        "#data['text']=data['text'].astype(str).apply(rem_stopwords)\n",
        "#data['text']=data['text'].astype(str).apply(rem_punct)\n",
        "#data['text']=data['text'].astype(str).apply(to_number)\n",
        "#data['text']=data['text'].astype(str).apply(lemmatize_words)\n",
        "print(data['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxwpG1iTETqW",
        "outputId": "cd73a6b9-6d80-4d04-9d96-ff300aa2bd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      1 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 ...\n",
            "1      2 1 Cumings, Mrs. John Bradley (Florence Brigg...\n",
            "2      3 3 Heikkinen, Miss. Laina female 26.0 0 0 STO...\n",
            "3      4 1 Futrelle, Mrs. Jacques Heath (Lily May Pee...\n",
            "4      5 3 Allen, Mr. William Henry male 35.0 0 0 373...\n",
            "                             ...                        \n",
            "886    887 2 Montvila, Rev. Juozas male 27.0 0 0 2115...\n",
            "887    888 1 Graham, Miss. Margaret Edith female 19.0...\n",
            "888    889 3 Johnston, Miss. Catherine Helen \"Carrie\"...\n",
            "889    890 1 Behr, Mr. Karl Howell male 26.0 0 0 1113...\n",
            "890    891 3 Dooley, Mr. Patrick male 32.0 0 0 370376...\n",
            "Name: text, Length: 891, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2d8f221fc580>:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Digit Conversion"
      ],
      "metadata": {
        "id": "sjA5uoZtg3Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_words(text):\n",
        "    word_map = {\n",
        "        '0': 'zero',\n",
        "        '1': 'one',\n",
        "        '2': 'two',\n",
        "        '3': 'three',\n",
        "        '4': 'four',\n",
        "        '5': 'five',\n",
        "        '6': 'six',\n",
        "        '7': 'seven',\n",
        "        '8': 'eight',\n",
        "        '9': 'nine'}\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "        converted_word = ''.join([word_map.get(char, char) for char in word]) #reads each caracter from the split in order to separate all digits\n",
        "        result.append(converted_word)\n",
        "    return ' '.join(result)\n",
        "data['text']=data['text'].apply(number_to_words)\n",
        "print(data['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6QkJm3Fg6Pk",
        "outputId": "bb63367b-41fe-4131-f75e-4807b0e32b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      one three Braund, Mr. Owen Harris male twotwo....\n",
            "1      two one Cumings, Mrs. John Bradley (Florence B...\n",
            "2      three three Heikkinen, Miss. Laina female twos...\n",
            "3      four one Futrelle, Mrs. Jacques Heath (Lily Ma...\n",
            "4      five three Allen, Mr. William Henry male three...\n",
            "                             ...                        \n",
            "886    eighteightseven two Montvila, Rev. Juozas male...\n",
            "887    eighteighteight one Graham, Miss. Margaret Edi...\n",
            "888    eighteightnine three Johnston, Miss. Catherine...\n",
            "889    eightninezero one Behr, Mr. Karl Howell male t...\n",
            "890    eightnineone three Dooley, Mr. Patrick male th...\n",
            "Name: text, Length: 891, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Creation"
      ],
      "metadata": {
        "id": "C6zUQngBowsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier as SGD\n",
        "from sklearn.svm import LinearSVC as SVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report as CR\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['Survived'], test_size=0.2, random_state=42)\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "QquKbI_2o7Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training for support vector machine\n",
        "svm=SVM(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "svm.fit(X_train_vec,y_train)\n",
        "#prediction\n",
        "y_pred = svm.predict(X_test_vec)\n",
        "print(CR(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVY7vTzTrrsW",
        "outputId": "08786550-492e-449a-9877-8b15f9dcc838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       105\n",
            "           1       0.83      0.74      0.79        74\n",
            "\n",
            "    accuracy                           0.83       179\n",
            "   macro avg       0.83      0.82      0.82       179\n",
            "weighted avg       0.83      0.83      0.83       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training for SGD\n",
        "sgd=SGD(loss='hinge',penalty='l2',max_iter=500,random_state=42)\n",
        "sgd.fit(X_train_vec,y_train)\n",
        "#prediction\n",
        "y_pred = sgd.predict(X_test_vec)\n",
        "print(CR(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpI3AuFG3ETg",
        "outputId": "0e7fcf43-d097-45a1-89e8-a0e52c4603b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       105\n",
            "           1       0.83      0.77      0.80        74\n",
            "\n",
            "    accuracy                           0.84       179\n",
            "   macro avg       0.84      0.83      0.83       179\n",
            "weighted avg       0.84      0.84      0.84       179\n",
            "\n"
          ]
        }
      ]
    }
  ]
}